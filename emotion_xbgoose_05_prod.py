# -*- coding: utf-8 -*-
"""
–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π
–§–æ—Ä–º–∞—Ç: [timestamp][model1][model2][modelN] -> text
"""

import torch
import torch.nn.functional as F
import torchaudio
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Union, Optional, Tuple
from transformers import (
    HubertForSequenceClassification, 
    Wav2Vec2FeatureExtractor
)

logger = logging.getLogger(__name__)

class FixedCompactEmotionAnalyzer:
    """
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π
    """
    
    def __init__(self, device: Optional[str] = None, models_to_use: Optional[List[str]] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.all_models_config = {
            'dusha': {
                'name': 'DUSHA HuBERT',
                'short_name': 'DUSHA',
                'model_path': 'xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned',
                'feature_extractor': 'facebook/hubert-large-ls960-ft',
                'emotions': ['neutral', 'angry', 'positive', 'sad', 'other'],
                'emotions_ru': ['–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', '–ó–ª–æ—Å—Ç—å', '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è', '–ì—Ä—É—Å—Ç—å', '–ü—Ä–æ—á–µ–µ'],
            },
            'aniemore_hubert': {
                'name': 'Aniemore HuBERT',
                'short_name': 'Aniemore',
                'model_path': 'Aniemore/hubert-emotion-russian-resd',
                'feature_extractor': 'Aniemore/hubert-emotion-russian-resd',
                'emotions': ['anger', 'disgust', 'enthusiasm', 'fear', 'happiness', 'neutral', 'sadness'],
                'emotions_ru': ['–ó–ª–æ—Å—Ç—å', '–û—Ç–≤—Ä–∞—â–µ–Ω–∏–µ', '–≠–Ω—Ç—É–∑–∏–∞–∑–º', '–°—Ç—Ä–∞—Ö', '–°—á–∞—Å—Ç—å–µ', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', '–ì—Ä—É—Å—Ç—å'],
            }
        }
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π
        if models_to_use is None:
            self.models_to_use = list(self.all_models_config.keys())
        else:
            self.models_to_use = [m for m in models_to_use if m in self.all_models_config]
        
        self.models_config = {
            k: v for k, v in self.all_models_config.items() 
            if k in self.models_to_use
        }
        
        self._loaded_models = {}
        self.sample_rate = 16000
        self.max_audio_length = 10  # —Å–µ–∫—É–Ω–¥
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ {self.device}")
        logger.info(f"–ú–æ–¥–µ–ª–∏: {[config['short_name'] for config in self.models_config.values()]}")
    
    def load_model(self, model_key: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if model_key in self._loaded_models:
            return self._loaded_models[model_key]
        
        config = self.models_config[model_key]
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {config['name']}...")
        
        try:
            # Feature extractor
            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
                config['feature_extractor']
            )
            
            # –ú–æ–¥–µ–ª—å
            if model_key == 'dusha':
                model = HubertForSequenceClassification.from_pretrained(
                    config['model_path']
                )
            elif model_key == 'aniemore_hubert':
                model = HubertForSequenceClassification.from_pretrained(
                    config['model_path'],
                    num_labels=len(config['emotions']),
                    ignore_mismatched_sizes=True
                )
            
            model.to(self.device)
            model.eval()
            
            self._loaded_models[model_key] = {
                'model': model,
                'feature_extractor': feature_extractor,
                'config': config
            }
            
            logger.info(f"‚úÖ {config['name']} –≥–æ—Ç–æ–≤–∞")
            return self._loaded_models[model_key]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {config['name']}: {e}")
            raise
    
    def parse_timestamp(self, timestamp_str: str) -> Tuple[float, float]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–π–º—Å—Ç–∞–º–ø–∞"""
        pattern = r'\[(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})\]'
        match = re.match(pattern, timestamp_str.strip())
        
        if not match:
            raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ç–∞–π–º—Å—Ç–∞–º–ø–∞: {timestamp_str}")
        
        start_h, start_m, start_s, start_ms = map(int, match.groups()[:4])
        end_h, end_m, end_s, end_ms = map(int, match.groups()[4:])
        
        start_seconds = start_h * 3600 + start_m * 60 + start_s + start_ms / 1000
        end_seconds = end_h * 3600 + end_m * 60 + end_s + end_ms / 1000
        
        return start_seconds, end_seconds
    
    def parse_sentences_file(self, sentences_file: Union[str, Path]) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏"""
        sentences_file = Path(sentences_file)
        if not sentences_file.exists():
            raise FileNotFoundError(f"–§–∞–π–ª —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {sentences_file}")
        
        sentences = []
        
        with open(sentences_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                if ' -> ' in line:
                    timestamp_part, text_part = line.split(' -> ', 1)
                else:
                    logger.warning(f"–°—Ç—Ä–æ–∫–∞ {line_num} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ' -> ': {line}")
                    continue
                
                try:
                    start_time, end_time = self.parse_timestamp(timestamp_part)
                    sentences.append({
                        'timestamp': timestamp_part.strip(),
                        'start': start_time,
                        'end': end_time,
                        'text': text_part.strip(),
                        'line_number': line_num
                    })
                except ValueError as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∞–π–º—Å—Ç–∞–º–ø–∞ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: {e}")
                    continue
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
        return sentences
    
    def extract_audio_segment(self, audio_path: Union[str, Path], 
                             start_time: float, end_time: float) -> torch.Tensor:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –∞—É–¥–∏–æ"""
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        waveform, original_sample_rate = torchaudio.load(str(audio_path))
        
        # –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –¥–æ 16kHz
        if original_sample_rate != self.sample_rate:
            transform = torchaudio.transforms.Resample(original_sample_rate, self.sample_rate)
            waveform = transform(waveform)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–æ–Ω–æ
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞
        start_sample = int(start_time * self.sample_rate)
        end_sample = int(end_time * self.sample_rate)
        
        start_sample = max(0, start_sample)
        end_sample = min(waveform.shape[1], end_sample)
        
        segment = waveform[:, start_sample:end_sample]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        min_length = int(0.1 * self.sample_rate)
        if segment.shape[1] < min_length:
            padding = min_length - segment.shape[1]
            segment = torch.nn.functional.pad(segment, (0, padding))
        
        return segment
    
    def predict_segment_emotion(self, audio_segment: torch.Tensor, model_key: str) -> str:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
        try:
            model_data = self.load_model(model_key)
            model = model_data['model']
            feature_extractor = model_data['feature_extractor']
            config = model_data['config']
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –ò–°–ü–†–ê–í–õ–ï–ù–û!
            inputs = feature_extractor(
                audio_segment.squeeze().numpy(),
                sampling_rate=feature_extractor.sampling_rate,
                return_tensors="pt",
                padding=True,
                max_length=16000 * self.max_audio_length,  # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï!
                truncation=True
            )
            
            input_values = inputs['input_values'].to(self.device)
            
            # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
            with torch.no_grad():
                outputs = model(input_values)
                logits = outputs.logits
                predictions = torch.argmax(logits, dim=-1)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if self.device.type == "cuda":
                predictions = predictions.cpu()
                
            predicted_id = predictions.numpy()[0]
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ID
            num_emotions = len(config['emotions'])
            if predicted_id >= num_emotions:
                predicted_id = 0
            
            return config['emotions_ru'][predicted_id]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model_key}: {e}")
            return "–û—à–∏–±–∫–∞"
    
    def analyze_and_create_transcription(self, wav_filename: Union[str, Path], 
                                       output_file: Optional[Union[str, Path]] = None) -> Path:
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ —Å —ç–º–æ—Ü–∏—è–º–∏"""
        wav_path = Path(wav_filename)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if wav_path.suffix.lower() == '.wav':
            base_name = wav_path.stem
            audio_file = wav_path
        else:
            base_name = str(wav_path)
            audio_file = wav_path.with_suffix('.wav')
        
        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–∞ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
        sentences_file = wav_path.parent / f"{base_name}_sentences.txt"
        
        if not audio_file.exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file}")
        
        if not sentences_file.exists():
            raise FileNotFoundError(f"–§–∞–π–ª —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {sentences_file}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–º–µ–Ω–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if output_file is None:
            output_file = wav_path.parent / f"{base_name}_emotions.txt"
        
        output_file = Path(output_file)
        
        logger.info(f"üéµ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {audio_file.name}")
        logger.info(f"üìÑ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {sentences_file.name}")
        logger.info(f"üíæ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_file.name}")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = self.parse_sentences_file(sentences_file)
        
        if not sentences:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ {len(self.models_to_use)} –º–æ–¥–µ–ª–µ–π...")
        for model_key in self.models_to_use:
            self.load_model(model_key)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ —Å —ç–º–æ—Ü–∏—è–º–∏
        with open(output_file, 'w', encoding='utf-8') as f:
            # –ó–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
            model_names = [config['short_name'] for config in self.models_config.values()]
            f.write(f"# –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π | –ú–æ–¥–µ–ª–∏: {' | '.join(model_names)}\n")
            f.write(f"# –§–∞–π–ª: {audio_file.name}\n")
            f.write(f"# –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(sentences)}\n")
            f.write("\n")
            
            for i, sentence in enumerate(sentences, 1):
                logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞ {i}/{len(sentences)}")
                
                try:
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ—Å–µ–≥–º–µ–Ω—Ç–∞
                    audio_segment = self.extract_audio_segment(
                        audio_file, 
                        sentence['start'], 
                        sentence['end']
                    )
                    
                    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª—å—é
                    emotion_results = []
                    for model_key in self.models_to_use:
                        emotion = self.predict_segment_emotion(audio_segment, model_key)
                        emotion_results.append(emotion)
                    
                    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    emotion_brackets = ''.join(f"[{emotion}]" for emotion in emotion_results)
                    result_line = f"{sentence['timestamp']}{emotion_brackets} -> {sentence['text']}\n"
                    
                    f.write(result_line)
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    emotions_str = " | ".join(emotion_results)
                    logger.info(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç: {emotions_str}")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞ {i}: {e}")
                    # –ó–∞–ø–∏—Å—å –æ—à–∏–±–∫–∏ –≤ —Ñ–∞–π–ª
                    error_brackets = ''.join(f"[–û—à–∏–±–∫–∞]" for _ in self.models_to_use)
                    error_line = f"{sentence['timestamp']}{error_brackets} -> {sentence['text']}\n"
                    f.write(error_line)
        
        logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å —ç–º–æ—Ü–∏—è–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_file}")
        return output_file

# –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def create_emotion_transcription_fixed(wav_filename: Union[str, Path], 
                                     models: Optional[List[str]] = None,
                                     output_file: Optional[Union[str, Path]] = None) -> Path:
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ —Å —ç–º–æ—Ü–∏—è–º–∏
    
    Args:
        wav_filename: –ò–º—è WAV —Ñ–∞–π–ª–∞
        models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ['dusha', 'aniemore_hubert'] –∏–ª–∏ None –¥–ª—è –≤—Å–µ—Ö
        output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        
    Returns:
        –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    analyzer = FixedCompactEmotionAnalyzer(models_to_use=models)
    return analyzer.analyze_and_create_transcription(wav_filename, output_file)

def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    import argparse
    
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    parser = argparse.ArgumentParser(description='–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º')
    parser.add_argument('wav_file', help='–ü—É—Ç—å –∫ WAV —Ñ–∞–π–ª—É')
    parser.add_argument('-o', '--output', help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É', default=None)
    parser.add_argument('-m', '--models', nargs='+', 
                       choices=['dusha', 'aniemore_hubert'],
                       help='–ú–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è', default=None)
    
    args = parser.parse_args()
    
    try:
        output_file = create_emotion_transcription_fixed(args.wav_file, args.models, args.output)
        print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {output_file}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        with open(output_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"\nüìÑ –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:")
            for line in lines[:10]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                print(line.rstrip())
            if len(lines) > 10:
                print("...")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_wav = r"D:\Python\WhisperX_noGUI_w_02\output_5\–ú–∞—Ä–∏—è –∫–æ—É—á–∏–Ω–≥ 24.06.25.wav"
    
    if Path(test_wav).exists():
        try:
            logging.basicConfig(level=logging.INFO)
            output_file = create_emotion_transcription_fixed(test_wav)
            print(f"\n‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {output_file}")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            with open(output_file, 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"\nüìÑ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞:")
                print(content[:1000] + "..." if len(content) > 1000 else content)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    else:
        main()

        