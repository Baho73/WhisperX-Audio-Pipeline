#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
audio_pipeline_complete.py

–ü–û–õ–ù–´–ô –ú–û–ù–û–õ–ò–¢–ù–´–ô PIPELINE –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò –ê–£–î–ò–û
–í–∫–ª—é—á–∞–µ—Ç: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é, –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é, –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–í–µ—Ä—Å–∏—è: 2.0.0
–ê–≤—Ç–æ—Ä: AI Assistant
"""

import os
import sys
import gc
import re
import json
import torch
import torchaudio
import torch.nn.functional as F
import logging
import warnings
import subprocess
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore", category=UserWarning)
logging.getLogger("pyannote.audio").setLevel(logging.ERROR)

# ================================================================================================
# –ë–õ–û–ö –ù–ê–°–¢–†–û–ï–ö - –í–°–ï –ù–ê–°–¢–†–û–ô–ö–ò –í –û–î–ù–û–ú –ú–ï–°–¢–ï
# ================================================================================================

class Config:
    """–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—Å–µ–≥–æ pipeline"""
    
    # === –ü–£–¢–ò –ö –î–ò–†–ï–ö–¢–û–†–ò–Ø–ú ===
    INPUT_DIR = r"D:/Python/YT_DL/all"          # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏
    OUTPUT_DIR = r"./output_6"                     # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    MODEL_DIR = r"./models"                      # –ü–∞–ø–∫–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    
    # === –†–ï–ñ–ò–ú –û–ë–†–ê–ë–û–¢–ö–ò ===
    PROCESSING_MODE = 'DIARIZE'                  # 'DIARIZE' –∏–ª–∏ 'SPLIT_STEREO'
    
    # === HUGGING FACE –¢–û–ö–ï–ù ===
    # –ü–æ–ª—É—á–∏—Ç–µ –Ω–∞ https://huggingface.co/settings/tokens
    HF_TOKEN = 'hf_YOUR_TOKEN_HERE'             # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Ç–æ–∫–µ–Ω!
    
    # === –ù–ê–°–¢–†–û–ô–ö–ò WHISPER ===
    WHISPER_MODEL = "large-v3"                   # "large-v3", "medium", "small", "base"
    WHISPER_LANGUAGE = "ru"                      # –Ø–∑—ã–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    BATCH_SIZE = 4                                # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–º–µ–Ω—å—à–µ = –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏)
    COMPUTE_TYPE = "float16"                     # "float16" –∏–ª–∏ "int8"
    
    # === –ù–ê–°–¢–†–û–ô–ö–ò –≠–ú–û–¶–ò–ô ===
    ENABLE_EMOTIONS = True                       # –í–∫–ª—é—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
    EMOTION_MODELS = ['dusha', 'aniemore_hubert'] # –ö–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    
    # === –£–ü–†–ê–í–õ–ï–ù–ò–ï –ü–ê–ú–Ø–¢–¨–Æ ===
    CLEAR_MEMORY_EVERY = 2                       # –û—á–∏—â–∞—Ç—å –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ N —Ñ–∞–π–ª–æ–≤
    FORCE_RELOAD_MODELS = 5                      # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ N —Ñ–∞–π–ª–æ–≤
    
    # === –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
    AUTO_CREATE_HTML = True                      # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å HTML –≤–µ—Ä—Å–∏—é
    AUTO_CREATE_ALIGNED = True                   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
    SHOW_TERMINAL_PREVIEW = True                 # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–≤—å—é –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
    TERMINAL_PREVIEW_LINES = 10                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–µ–≤—å—é
    
    # === –†–ï–ñ–ò–ú SPLIT_STEREO (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) ===
    SPEAKER_LABEL_LEFT = "–°–ø–∏–∫–µ—Ä_1"
    SPEAKER_LABEL_RIGHT = "–°–ø–∏–∫–µ—Ä_2"
    
    # === –û–¢–õ–ê–î–ö–ê ===
    DEBUG_MODE = False                            # –í–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    SKIP_EXISTING = True                         # –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

# ================================================================================================
# EMOTION PROPERTIES AND DATA STRUCTURES
# ================================================================================================

@dataclass
class EmotionProperties:
    """–°–≤–æ–π—Å—Ç–≤–∞ —ç–º–æ—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
    color: str
    rich_color: str
    symbol: str

@dataclass
class Segment:
    """–°–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
    timestamp: str
    emotions: List[str]
    text: str
    line_number: Optional[int] = None

# –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ –¥–ª—è —ç–º–æ—Ü–∏–π
EMOTION_COLORS: Dict[str, EmotionProperties] = {
    '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è': EmotionProperties('#808080', 'bright_black', '‚óã'),
    '–ó–ª–æ—Å—Ç—å': EmotionProperties('#FF4444', 'red', '‚ñ≤'),
    '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è': EmotionProperties('#44FF44', 'green', '‚ô¶'),
    '–ì—Ä—É—Å—Ç—å': EmotionProperties('#4444FF', 'blue', '‚ñº'),
    '–ü—Ä–æ—á–µ–µ': EmotionProperties('#FFA500', 'yellow', '‚ñ†'),
    '–û—Ç–≤—Ä–∞—â–µ–Ω–∏–µ': EmotionProperties('#8B4513', 'magenta', '√ó'),
    '–≠–Ω—Ç—É–∑–∏–∞–∑–º': EmotionProperties('#FFD700', 'bright_yellow', '‚òÖ'),
    '–°—Ç—Ä–∞—Ö': EmotionProperties('#9370DB', 'bright_magenta', '!'),
    '–°—á–∞—Å—Ç—å–µ': EmotionProperties('#00FF00', 'bright_green', '‚ô•'),
    '–û—à–∏–±–∫–∞': EmotionProperties('#FF0000', 'bright_red', '‚úó'),
}

# ================================================================================================
# –ò–ú–ü–û–†–¢–´ –ú–û–î–ï–õ–ï–ô (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
# ================================================================================================

# WhisperX
whisperx = None
# Pyannote
Pipeline = None
# Transformers –¥–ª—è —ç–º–æ—Ü–∏–π
HubertForSequenceClassification = None
Wav2Vec2FeatureExtractor = None
# Rich –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
Console = None
Table = None
RICH_AVAILABLE = False

def lazy_imports():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç—è–∂–µ–ª—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫"""
    global whisperx, Pipeline, HubertForSequenceClassification, Wav2Vec2FeatureExtractor
    global Console, Table, RICH_AVAILABLE
    
    try:
        import whisperx as wx
        whisperx = wx
    except ImportError:
        print("‚ùå WhisperX –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install whisperx")
        sys.exit(1)
    
    try:
        from pyannote.audio import Pipeline as PA_Pipeline
        Pipeline = PA_Pipeline
    except ImportError:
        print("‚ùå Pyannote –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pyannote.audio")
        sys.exit(1)
    
    try:
        from transformers import (
            HubertForSequenceClassification as HubertModel,
            Wav2Vec2FeatureExtractor as WavExtractor
        )
        HubertForSequenceClassification = HubertModel
        Wav2Vec2FeatureExtractor = WavExtractor
    except ImportError:
        if Config.ENABLE_EMOTIONS:
            print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –≠–º–æ—Ü–∏–∏ –±—É–¥—É—Ç –æ—Ç–∫–ª—é—á–µ–Ω—ã")
            Config.ENABLE_EMOTIONS = False
    
    try:
        from rich.console import Console as RichConsole
        from rich.table import Table as RichTable
        Console = RichConsole
        Table = RichTable
        RICH_AVAILABLE = True
    except ImportError:
        print("‚ÑπÔ∏è Rich –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# ================================================================================================
# –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
# ================================================================================================

def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    level = logging.DEBUG if Config.DEBUG_MODE else logging.INFO
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    logging.basicConfig(level=level, format=log_format, stream=sys.stdout)
    return logging.getLogger(__name__)

logger = setup_logging()

# ================================================================================================
# EMOTION ANALYZER
# ================================================================================================

class EmotionAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π –¥–ª—è –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, device=None, models_to_use=None):
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        self.all_models_config = {
            'dusha': {
                'name': 'DUSHA HuBERT',
                'short_name': 'DUSHA',
                'model_path': 'xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned',
                'feature_extractor': 'facebook/hubert-large-ls960-ft',
                'emotions': ['neutral', 'angry', 'positive', 'sad', 'other'],
                'emotions_ru': ['–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', '–ó–ª–æ—Å—Ç—å', '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è', '–ì—Ä—É—Å—Ç—å', '–ü—Ä–æ—á–µ–µ'],
            },
            'aniemore_hubert': {
                'name': 'Aniemore HuBERT',
                'short_name': 'Aniemore',
                'model_path': 'Aniemore/hubert-emotion-russian-resd',
                'feature_extractor': 'Aniemore/hubert-emotion-russian-resd',
                'emotions': ['anger', 'disgust', 'enthusiasm', 'fear', 'happiness', 'neutral', 'sadness'],
                'emotions_ru': ['–ó–ª–æ—Å—Ç—å', '–û—Ç–≤—Ä–∞—â–µ–Ω–∏–µ', '–≠–Ω—Ç—É–∑–∏–∞–∑–º', '–°—Ç—Ä–∞—Ö', '–°—á–∞—Å—Ç—å–µ', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', '–ì—Ä—É—Å—Ç—å'],
            }
        }
        
        if models_to_use is None:
            self.models_to_use = list(self.all_models_config.keys())
        else:
            self.models_to_use = [m for m in models_to_use if m in self.all_models_config]
        
        self.models_config = {
            k: v for k, v in self.all_models_config.items() 
            if k in self.models_to_use
        }
        
        self._loaded_models = {}
        self.sample_rate = 16000
        self.max_audio_length = 10
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
    
    def load_model(self, model_key):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–æ—Ü–∏–π"""
        if model_key in self._loaded_models:
            return self._loaded_models[model_key]
        
        config = self.models_config[model_key]
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {config['name']}...")
        
        try:
            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
                config['feature_extractor']
            )
            
            if model_key == 'dusha':
                model = HubertForSequenceClassification.from_pretrained(
                    config['model_path']
                )
            elif model_key == 'aniemore_hubert':
                model = HubertForSequenceClassification.from_pretrained(
                    config['model_path'],
                    num_labels=len(config['emotions']),
                    ignore_mismatched_sizes=True
                )
            
            model.to(self.device)
            model.eval()
            
            self._loaded_models[model_key] = {
                'model': model,
                'feature_extractor': feature_extractor,
                'config': config
            }
            
            logger.info(f"‚úÖ {config['name']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return self._loaded_models[model_key]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {config['name']}: {e}")
            raise
    
    def extract_audio_segment(self, audio_path, start_time, end_time):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –∞—É–¥–∏–æ"""
        waveform, original_sample_rate = torchaudio.load(str(audio_path))
        
        if original_sample_rate != self.sample_rate:
            transform = torchaudio.transforms.Resample(original_sample_rate, self.sample_rate)
            waveform = transform(waveform)
        
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        start_sample = int(start_time * self.sample_rate)
        end_sample = int(end_time * self.sample_rate)
        
        start_sample = max(0, start_sample)
        end_sample = min(waveform.shape[1], end_sample)
        
        segment = waveform[:, start_sample:end_sample]
        
        min_length = int(0.1 * self.sample_rate)
        if segment.shape[1] < min_length:
            padding = min_length - segment.shape[1]
            segment = F.pad(segment, (0, padding))
        
        return segment
    
    def predict_segment_emotion(self, audio_segment, model_key):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
        try:
            model_data = self.load_model(model_key)
            model = model_data['model']
            feature_extractor = model_data['feature_extractor']
            config = model_data['config']
            
            inputs = feature_extractor(
                audio_segment.squeeze().numpy(),
                sampling_rate=feature_extractor.sampling_rate,
                return_tensors="pt",
                padding=True,
                max_length=16000 * self.max_audio_length,
                truncation=True
            )
            
            input_values = inputs['input_values'].to(self.device)
            
            with torch.no_grad():
                outputs = model(input_values)
                logits = outputs.logits
                predictions = torch.argmax(logits, dim=-1)
            
            if self.device.type == "cuda":
                predictions = predictions.cpu()
                
            predicted_id = predictions.numpy()[0]
            
            num_emotions = len(config['emotions'])
            if predicted_id >= num_emotions:
                predicted_id = 0
            
            return config['emotions_ru'][predicted_id]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model_key}: {e}")
            return "–û—à–∏–±–∫–∞"
    
    def clear_models(self):
        """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        for model_key in list(self._loaded_models.keys()):
            del self._loaded_models[model_key]
        self._loaded_models = {}
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ================================================================================================
# FORMATTER
# ================================================================================================

class ResultFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    def __init__(self):
        self.console = Console() if RICH_AVAILABLE else None
        self.emotion_stats = {}
    
    def find_max_emotion_length(self, emotions_list):
        """–ù–∞—Ö–æ–¥–∏—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —ç–º–æ—Ü–∏–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è"""
        max_lengths = {}
        for emotions in emotions_list:
            for i, emotion in enumerate(emotions):
                if i not in max_lengths:
                    max_lengths[i] = 0
                max_lengths[i] = max(max_lengths[i], len(emotion))
        return max_lengths
    
    def create_html(self, segments, output_file, source_file=""):
        """–°–æ–∑–¥–∞–µ—Ç HTML —Ñ–∞–π–ª —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —ç–º–æ—Ü–∏–π"""
        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π - {source_file}</title>
    <style>
        body {{
            font-family: 'Consolas', 'Monaco', monospace;
            background-color: #1e1e1e;
            color: #d4d4d4;
            padding: 20px;
            line-height: 1.6;
        }}
        h1 {{
            color: #569cd6;
            border-bottom: 2px solid #3e3e3e;
            padding-bottom: 10px;
        }}
        .timestamp {{
            color: #569cd6;
            font-weight: bold;
        }}
        .emotion {{
            padding: 2px 6px;
            margin: 0 2px;
            border-radius: 3px;
            font-weight: bold;
            display: inline-block;
        }}
        .text {{
            color: #ffffff;
            margin-left: 10px;
        }}
        .segment {{
            margin: 5px 0;
            padding: 8px;
            border-left: 3px solid #3e3e3e;
            transition: all 0.2s;
        }}
        .segment:hover {{
            background-color: #2d2d2d;
            border-left-color: #569cd6;
        }}
        .legend {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
            padding: 15px;
            background-color: #2d2d2d;
            border-radius: 5px;
        }}
        .legend-item {{
            display: flex;
            align-items: center;
            gap: 5px;
        }}
    </style>
</head>
<body>
    <h1>üé≠ –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π</h1>
    <p>–§–∞–π–ª: {source_file} | –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}</p>
    
    <div class="legend">
        <strong style="width: 100%; color: #569cd6;">–õ–µ–≥–µ–Ω–¥–∞ —ç–º–æ—Ü–∏–π:</strong>"""
        
        for emotion, props in EMOTION_COLORS.items():
            html += f"""
        <div class="legend-item">
            <span class="emotion" style="background-color: {props.color};">
                {props.symbol} {emotion}
            </span>
        </div>"""
        
        html += """
    </div>
    <div class="segments">"""
        
        for seg in segments:
            html += f"""
        <div class="segment">
            <span class="timestamp">{seg['timestamp']}</span>"""
            
            for emotion in seg.get('emotions', []):
                emotion_clean = emotion.strip()
                props = EMOTION_COLORS.get(emotion_clean, EmotionProperties('#808080', 'white', '‚óè'))
                html += f"""
            <span class="emotion" style="background-color: {props.color};">
                {props.symbol} {emotion_clean}
            </span>"""
            
            html += f"""
            <span class="text">‚Üí {seg['text']}</span>
        </div>"""
        
        html += """
    </div>
</body>
</html>"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)
        
        logger.info(f"HTML —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {output_file}")
    
    def create_aligned_text(self, segments, output_file):
        """–°–æ–∑–¥–∞–µ—Ç –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"""
        all_emotions = [seg.get('emotions', []) for seg in segments]
        max_lengths = self.find_max_emotion_length(all_emotions)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∞–Ω–∞–ª–∏–∑–æ–º —ç–º–æ—Ü–∏–π (–≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\n")
            f.write(f"# –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}\n\n")
            
            for seg in segments:
                aligned_emotions = ''.join(
                    f"[{emotion.ljust(max_lengths.get(i, 11))}]"
                    for i, emotion in enumerate(seg.get('emotions', []))
                )
                f.write(f"{seg['timestamp']}{aligned_emotions} -> {seg['text']}\n")
        
        logger.info(f"–í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {output_file}")
    
    def show_terminal_preview(self, segments, limit=10):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–≤—å—é –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ"""
        if not RICH_AVAILABLE or not self.console:
            # –ü—Ä–æ—Å—Ç–æ–π –≤—ã–≤–æ–¥ –±–µ–∑ Rich
            print("\n=== –ü—Ä–µ–≤—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===")
            for i, seg in enumerate(segments[:limit]):
                print(f"{seg['timestamp']} {seg.get('emotions', [])} -> {seg['text'][:80]}...")
            if len(segments) > limit:
                print(f"... –∏ –µ—â–µ {len(segments) - limit} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            return
        
        # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Å Rich
        table = Table(title="üé≠ –ü—Ä–µ–≤—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞")
        table.add_column("–í—Ä–µ–º—è", style="cyan", no_wrap=True)
        table.add_column("–≠–º–æ—Ü–∏–∏", style="magenta")
        table.add_column("–¢–µ–∫—Å—Ç", style="white")
        
        for seg in segments[:limit]:
            time_clean = seg['timestamp'].strip('[]')
            emotions_str = ' | '.join(seg.get('emotions', []))
            text = seg['text'][:60] + "..." if len(seg['text']) > 60 else seg['text']
            table.add_row(time_clean, emotions_str, text)
        
        self.console.print(table)
        if len(segments) > limit:
            self.console.print(f"[yellow]–ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {limit} –∏–∑ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤[/yellow]")

# ================================================================================================
# AUDIO PROCESSING
# ================================================================================================

def clear_gpu_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def convert_to_wav_if_needed(input_path, output_dir):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
    file_ext = Path(input_path).suffix.lower()
    formats_to_convert = ['.m4a', '.mp3', '.aac', '.flac', '.ogg']
    
    if file_ext in formats_to_convert:
        base_name = Path(input_path).stem
        wav_path = os.path.join(output_dir, f"{base_name}_converted.wav")
        logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {file_ext} –≤ WAV...")
        
        command = [
            "ffmpeg", "-y", "-i", input_path,
            "-ar", "16000", "-ac", "1", "-c:a", "pcm_s16le",
            wav_path, "-loglevel", "error"
        ]
        
        try:
            subprocess.run(command, check=True)
            logger.info(f"–§–∞–π–ª —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {wav_path}")
            return wav_path, True
        except subprocess.CalledProcessError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
            raise
    
    return input_path, False

def parse_timestamp(timestamp_str):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–π–º—Å—Ç–∞–º–ø–∞"""
    pattern = r'\[(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})\]'
    match = re.match(pattern, timestamp_str.strip())
    if not match:
        raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {timestamp_str}")
    
    start_h, start_m, start_s, start_ms = map(int, match.groups()[:4])
    end_h, end_m, end_s, end_ms = map(int, match.groups()[4:])
    
    start_seconds = start_h * 3600 + start_m * 60 + start_s + start_ms / 1000
    end_seconds = end_h * 3600 + end_m * 60 + end_s + end_ms / 1000
    
    return start_seconds, end_seconds

# ================================================================================================
# MAIN PROCESSING PIPELINE
# ================================================================================================

class AudioProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.models_loaded = False
        self.model_tx = None
        self.model_align = None
        self.metadata = None
        self.pipeline_diarize = None
        self.emotion_analyzer = None
        self.formatter = ResultFormatter()
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        if self.models_loaded:
            return
        
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        
        # WhisperX
        self.model_tx = whisperx.load_model(
            Config.WHISPER_MODEL, 
            self.device, 
            compute_type=Config.COMPUTE_TYPE, 
            download_root=Config.MODEL_DIR
        )
        self.model_align, self.metadata = whisperx.load_align_model(
            language_code=Config.WHISPER_LANGUAGE, 
            device=self.device, 
            model_dir=Config.MODEL_DIR
        )
        
        # Pyannote –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        if Config.PROCESSING_MODE == 'DIARIZE':
            self.pipeline_diarize = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1", 
                use_auth_token=Config.HF_TOKEN
            )
            self.pipeline_diarize.to(torch.device(self.device))
        
        # –≠–º–æ—Ü–∏–∏
        if Config.ENABLE_EMOTIONS:
            self.emotion_analyzer = EmotionAnalyzer(
                device=self.device, 
                models_to_use=Config.EMOTION_MODELS
            )
            for model_key in self.emotion_analyzer.models_to_use:
                self.emotion_analyzer.load_model(model_key)
        
        self.models_loaded = True
        logger.info("–í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    def process_audio(self, input_path):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞"""
        base_name = Path(input_path).stem
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
        final_file = Path(Config.OUTPUT_DIR) / f"{base_name}_final.txt"
        if Config.SKIP_EXISTING and final_file.exists():
            logger.info(f"–§–∞–π–ª —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {base_name}")
            return str(final_file)
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {base_name}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        wav_path, was_converted = convert_to_wav_if_needed(input_path, Config.OUTPUT_DIR)
        
        try:
            # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            logger.info("–≠—Ç–∞–ø 1/3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è...")
            sentences_file = Path(Config.OUTPUT_DIR) / f"{base_name}_sentences.txt"
            
            audio = whisperx.load_audio(input_path)
            result = self.model_tx.transcribe(
                audio, 
                batch_size=Config.BATCH_SIZE, 
                language=Config.WHISPER_LANGUAGE
            )
            aligned_result = whisperx.align(
                result["segments"], 
                self.model_align, 
                self.metadata, 
                audio, 
                self.device, 
                return_char_alignments=False
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            segments = []
            with open(sentences_file, "w", encoding="utf-8") as f:
                for segment in aligned_result["segments"]:
                    start, end, text = segment['start'], segment['end'], segment['text'].strip()
                    if not text:
                        continue
                    start_str = f"{int(start // 3600):02}:{int((start % 3600) // 60):02}:{start % 60:06.3f}"
                    end_str = f"{int(end // 3600):02}:{int((end % 3600) // 60):02}:{end % 60:06.3f}"
                    timestamp = f"[{start_str} - {end_str}]"
                    f.write(f"{timestamp} -> {text}\n")
                    segments.append({
                        'timestamp': timestamp,
                        'start': start,
                        'end': end,
                        'text': text,
                        'emotions': []
                    })
            
            del audio, result, aligned_result
            gc.collect()
            
            # 2. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if Config.PROCESSING_MODE == 'DIARIZE' and self.pipeline_diarize:
                logger.info("–≠—Ç–∞–ø 2/3: –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è...")
                diarization_result = self.pipeline_diarize(wav_path)
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
                del diarization_result
                gc.collect()
            
            # 3. –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
            if Config.ENABLE_EMOTIONS and self.emotion_analyzer:
                logger.info("–≠—Ç–∞–ø 3/3: –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π...")
                audio_file = Path(wav_path) if was_converted else Path(input_path)
                
                for i, seg in enumerate(segments, 1):
                    if i % 10 == 0:
                        logger.info(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {i}/{len(segments)}")
                    
                    try:
                        audio_segment = self.emotion_analyzer.extract_audio_segment(
                            audio_file, seg['start'], seg['end']
                        )
                        
                        emotions = []
                        for model_key in self.emotion_analyzer.models_to_use:
                            emotion = self.emotion_analyzer.predict_segment_emotion(
                                audio_segment, model_key
                            )
                            emotions.append(emotion)
                        
                        seg['emotions'] = emotions
                        
                        if i % 10 == 0:
                            del audio_segment
                            gc.collect()
                            
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–π: {e}")
                        seg['emotions'] = ['–û—à–∏–±–∫–∞'] * len(self.emotion_analyzer.models_to_use)
            
            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            with open(final_file, 'w', encoding='utf-8') as f:
                if Config.ENABLE_EMOTIONS:
                    model_names = [cfg['short_name'] 
                                 for cfg in self.emotion_analyzer.models_config.values()]
                    f.write(f"# –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π | –ú–æ–¥–µ–ª–∏: {' | '.join(model_names)}\n")
                else:
                    f.write("# –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è\n")
                f.write(f"# –§–∞–π–ª: {base_name}\n")
                f.write(f"# –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}\n\n")
                
                for seg in segments:
                    emotions_str = ''.join(f"[{e}]" for e in seg['emotions'])
                    f.write(f"{seg['timestamp']}{emotions_str} -> {seg['text']}\n")
            
            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª: {final_file}")
            
            # 5. –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            if Config.AUTO_CREATE_HTML:
                html_file = Path(Config.OUTPUT_DIR) / f"{base_name}_colored.html"
                self.formatter.create_html(segments, html_file, base_name)
            
            if Config.AUTO_CREATE_ALIGNED:
                aligned_file = Path(Config.OUTPUT_DIR) / f"{base_name}_aligned.txt"
                self.formatter.create_aligned_text(segments, aligned_file)
            
            if Config.SHOW_TERMINAL_PREVIEW:
                self.formatter.show_terminal_preview(segments, Config.TERMINAL_PREVIEW_LINES)
            
            return str(final_file)
            
        finally:
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ WAV —Ñ–∞–π–ª–∞
            if was_converted and os.path.exists(wav_path):
                os.remove(wav_path)
                logger.info("–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
    
    def process_directory(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        audio_extensions = ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.aac']
        audio_files = []
        
        for ext in audio_extensions:
            audio_files.extend(Path(Config.INPUT_DIR).glob(f"*{ext}"))
        
        if not audio_files:
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –≤ {Config.INPUT_DIR}")
            return
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(audio_files)}")
        
        processed = 0
        for i, audio_file in enumerate(audio_files, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"–§–∞–π–ª {i}/{len(audio_files)}: {audio_file.name}")
            logger.info('='*60)
            
            try:
                self.process_audio(str(audio_file))
                processed += 1
                
                # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é
                if processed % Config.CLEAR_MEMORY_EVERY == 0:
                    logger.info("–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
                    clear_gpu_memory()
                
                if Config.FORCE_RELOAD_MODELS > 0 and processed % Config.FORCE_RELOAD_MODELS == 0:
                    logger.info("–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
                    self.reload_models()
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {audio_file.name}: {e}")
                if Config.DEBUG_MODE:
                    import traceback
                    traceback.print_exc()
        
        logger.info(f"\n{'='*60}")
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed}/{len(audio_files)} —Ñ–∞–π–ª–æ–≤")
    
    def reload_models(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        self.models_loaded = False
        
        if self.model_tx:
            del self.model_tx
        if self.model_align:
            del self.model_align
        if self.metadata:
            del self.metadata
        if self.pipeline_diarize:
            del self.pipeline_diarize
        if self.emotion_analyzer:
            self.emotion_analyzer.clear_models()
            del self.emotion_analyzer
        
        clear_gpu_memory()
        self.load_models()

# ================================================================================================
# MAIN ENTRY POINT
# ================================================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    parser = argparse.ArgumentParser(
        description='–ü–æ–ª–Ω—ã–π pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--input-dir', 
        default=Config.INPUT_DIR,
        help='–ü–∞–ø–∫–∞ —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏'
    )
    parser.add_argument(
        '--output-dir', 
        default=Config.OUTPUT_DIR,
        help='–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'
    )
    parser.add_argument(
        '--batch-size', 
        type=int,
        default=Config.BATCH_SIZE,
        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞'
    )
    parser.add_argument(
        '--no-emotions', 
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π'
    )
    parser.add_argument(
        '--no-html', 
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å HTML —Ñ–∞–π–ª—ã'
    )
    parser.add_argument(
        '--debug', 
        action='store_true',
        help='–†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏'
    )
    parser.add_argument(
        '--single-file',
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ñ–∞–π–ª'
    )
    
    args = parser.parse_args()
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.input_dir:
        Config.INPUT_DIR = args.input_dir
    if args.output_dir:
        Config.OUTPUT_DIR = args.output_dir
    if args.batch_size:
        Config.BATCH_SIZE = args.batch_size
    if args.no_emotions:
        Config.ENABLE_EMOTIONS = False
    if args.no_html:
        Config.AUTO_CREATE_HTML = False
    if args.debug:
        Config.DEBUG_MODE = True
        setup_logging()  # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –ª–æ–≥–≥–µ—Ä —Å DEBUG —É—Ä–æ–≤–Ω–µ–º
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    os.makedirs(Config.INPUT_DIR, exist_ok=True)
    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
    os.makedirs(Config.MODEL_DIR, exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞
    if Config.PROCESSING_MODE == 'DIARIZE' and '–í–ê–®_–¢–û–ö–ï–ù' in Config.HF_TOKEN:
        logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω Hugging Face —Ç–æ–∫–µ–Ω!")
        logger.info("–ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω –Ω–∞ https://huggingface.co/settings/tokens")
        logger.info("–ò –∑–∞–º–µ–Ω–∏—Ç–µ 'hf_–í–ê–®_–¢–û–ö–ï–ù_–ó–î–ï–°–¨' –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö Config.HF_TOKEN")
        return 1
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    logger.info("="*60)
    logger.info("–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø:")
    logger.info(f"  –í—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {Config.INPUT_DIR}")
    logger.info(f"  –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {Config.OUTPUT_DIR}")
    logger.info(f"  –ú–æ–¥–µ–ª—å Whisper: {Config.WHISPER_MODEL}")
    logger.info(f"  –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {Config.BATCH_SIZE}")
    logger.info(f"  –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π: {'–î–∞' if Config.ENABLE_EMOTIONS else '–ù–µ—Ç'}")
    logger.info(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    logger.info("="*60)
    
    # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫...")
    lazy_imports()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    processor = AudioProcessor()
    processor.load_models()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    if args.single_file:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if not os.path.exists(args.single_file):
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.single_file}")
            return 1
        processor.process_audio(args.single_file)
    else:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        processor.process_directory()
    
    logger.info("\n‚ú® –ì–æ—Ç–æ–≤–æ!")
    return 0

if __name__ == "__main__":
    sys.exit(main())